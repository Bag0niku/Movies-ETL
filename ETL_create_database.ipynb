{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automate the Cleaning Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Dependencies required for the uploading process\n",
    "import time\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "from config import postgres_password\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions created during the initial cleaning\n",
    "- Each one takes a string as input:\n",
    " - cleaning_movie\n",
    "     - reduces the sheer variety of keys the movie object has in wiki_movie\n",
    " - find_dollars\n",
    "     - finds the dollar values hidden amoung the strings and lists\n",
    " - parse_dollars\n",
    "     - converts the dollar values found by find_dollars into numeric values\n",
    " - find_dates\n",
    "     - finds dates hidden in strings and lists to be converted to datetime objects with pd.to_datetime\n",
    " - find_time\n",
    "     - finds the movie duration in minutes from amoung the strings and lists and converts them into numeric values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cleaning_movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_movie(movie):\n",
    "    \"\"\"wiki_movie_file is a json file containing a list of dictionaries with each index as a different movie \n",
    "    with different key:value pairs. This function is specially designed to clean that specific dataset. \n",
    "    Wikipedia is managed by many people and they do not all use the same words.\n",
    "    \"\"\"\n",
    "    alternate_titles = dict()\n",
    "    fixed_movie = dict(movie)\n",
    "###  Start with alternate titles stored in language keys and merge them into one.\n",
    "###  Language keys i could find:      \n",
    "    language_keys = ['Also known as','Arabic','Cantonese','Chinese','French', 'Hangul','Hebrew','Hepburn','Japanese','Literally',\n",
    "        'Mandarin','McCune–Reischauer','Original title','Polish', 'Revised Romanization','Romanized','Russian',\n",
    "        'Simplified','Traditional','Yiddish']\n",
    "    for key in language_keys:\n",
    "        if key in fixed_movie.keys():\n",
    "            alternate_titles[key] = fixed_movie[key]\n",
    "            fixed_movie.pop(key)\n",
    "        else:\n",
    "            pass\n",
    "##  if there were alternate titles, add them to the movie.       \n",
    "    if len(alternate_titles) > 0:\n",
    "        fixed_movie['alternate_titles'] = alternate_titles\n",
    "    else:\n",
    "        pass\n",
    "        \n",
    "###  Alternative titles are fixed, now merge columns that are similar.     \n",
    "    keys_to_merge = {'Director':'Directed by', 'Country': 'Country of origin', 'Distributor(s)':'Distributed by',\n",
    "                     'Editor(s)':'Edited by',  'Language':'Original language(s)', 'Producer(s)':'Produced by',\n",
    "                     'Genre(s)': 'Genre', 'Composer(s)': ['Music by', 'Theme music composer'], \n",
    "                     'Release date': ['Release(s)', 'Original release'], 'Distributor(s)':['Distributed by','Distributor'],\n",
    "                     'Writer(s)':['Written by', 'Story by', 'Screenplay by', 'Screen story by', 'Adaptation by'],\n",
    "                     'Production Comapany': ['Production company', 'Production company(s)', 'Productioncompanies ','Productioncompany ']  }\n",
    "##  item = key, from the key:value pair and the key I want; values = the movie key(s) I do not want. \n",
    "    for item in keys_to_merge: \n",
    "        if type(keys_to_merge[item]) == type(list()):\n",
    "            for n in keys_to_merge[item]:\n",
    "                if n in fixed_movie.keys():\n",
    "                    fixed_movie[item] = fixed_movie.pop(n)\n",
    "                else:\n",
    "                    pass\n",
    "        else: \n",
    "            if keys_to_merge[item] in fixed_movie.keys():\n",
    "                fixed_movie[item] = fixed_movie.pop(keys_to_merge[item])\n",
    "            else:\n",
    "                pass\n",
    "    \n",
    "    return fixed_movie\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find_dollars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_dollars(s):\n",
    "    # declared patterns\n",
    "    # searching for pattern that is similar to: \"$45.3 million/billion\"\n",
    "    p1 = r'(\\$\\s?\\d+\\.?\\d*\\s*[bm]illi?on)'      \n",
    "    # searching for pattern that is similar to $123,456,789.0\n",
    "    p2 = r'(\\$\\s?\\d+(?:[,\\.]\\d{3})+\\.?\\d*)(?!\\s*[bm]illi?on)' \n",
    "    # modified pattern #1; includes a range $43.5-45.7 Million\n",
    "    p3 = r\"(\\$\\d*\\.?\\d*)(?:[-—–]\\d*\\.?\\d*)(\\s[bm]illi?on)\" # modified pattern #1 version 3\n",
    "    # searcing for $100 to $999999 (no commas)\n",
    "    p4 = r\"(\\$\\d{3,6})\" \n",
    "    if len(re.findall(p1, str(s), flags=re.IGNORECASE)) == 1:\n",
    "        num = re.findall(p1, str(s), flags=re.IGNORECASE)[0]\n",
    "        return num\n",
    "    elif len(re.findall(p2, str(s), flags=re.IGNORECASE)) == 1:\n",
    "        num = re.findall(p2, str(s), flags=re.IGNORECASE)[0]\n",
    "        return num\n",
    "    elif len(re.findall(p1, str(s), flags=re.IGNORECASE)) > 1: \n",
    "        num = re.findall(p1, str(s), flags=re.IGNORECASE)[1]\n",
    "        return num\n",
    "    elif len(re.findall(p2, str(s), flags=re.IGNORECASE)) > 1:  \n",
    "        num = re.findall(p2, str(s), flags=re.IGNORECASE)[1]\n",
    "        return num\n",
    "    elif len(re.findall(p3, str(s), flags=re.IGNORECASE)) > 0:\n",
    "        fix = re.findall(p3, str(s), flags=re.IGNORECASE)\n",
    "        num = fix[0][0] + fix[0][1]\n",
    "        return num\n",
    "    elif len(re.findall(p4, str(s), flags=re.IGNORECASE)) > 0:\n",
    "        num = re.findall(p4, str(s), flags=re.IGNORECASE)[0].replace(\" \", \"\")\n",
    "        return num\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parse_dollars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created during initial cleaning for budget and revenue/box office\n",
    "# parse the dollar values found with find_dollars() and durn them into numeric values\n",
    "def parse_dollars(s):   \n",
    "    # patterns to change into numbers\n",
    "    p1 = \"(\\$\\d+\\.?\\d*\\s*milli?on)\" # pulled from pattern #1 above\n",
    "    p2 = \"(\\$\\d+\\.?\\d*\\s*billi?on)\" # pulled from pattern #1 above\n",
    "    p3 = \"(\\$\\d+(?:[,\\.]\\d{3})+\\.?\\d*)\"  # pulled from pattern #2 above\n",
    "    \n",
    "    # if s is string, then change it, otherwise np.nan\n",
    "    if type(s) == str:\n",
    "  \n",
    "        # if input is of the form $###.## Million, pattern #1\n",
    "        if re.match(p1, s, flags=re.IGNORECASE):\n",
    "            # Remove dollar sign and \"million\", convert to float and multiply by 1,000,000\n",
    "            num = float(re.search(\"(\\d+\\.?\\d*)\", s)[0])*1000000\n",
    "            \n",
    "            # return value\n",
    "            return num\n",
    "            \n",
    "        # elif input is of the form $###.## Billion, formerly pattern #1 also, now pattern #2\n",
    "        elif re.match(p2, s, flags=re.IGNORECASE):\n",
    "        # Remove dollar sign and \"billion\", convert to float and multiply by 1,000,000,000\n",
    "            num = float(re.search(\"(\\d+\\.?\\d*)\", s)[0])*1000000000\n",
    "            # return value\n",
    "            return num\n",
    "\n",
    "            \n",
    "        # elif input is of the form $###,###,###, formerly pattern #2, now pattern #3\n",
    "        elif re.match(p3, s, flags=re.IGNORECASE):\n",
    "            # Remove dollar sign and commas, convert to float\n",
    "            num = float(re.findall(r\"(?:\\$)(\\d+(?:[,\\.]\\d{3})+\\.?\\d*)\", s)[0].replace(\",\", \"\"))\n",
    "\n",
    "            # return value\n",
    "            return num\n",
    "        else: \n",
    "            if len(s) <= 6:\n",
    "                if re.match(\"(\\$\\d{1,6}$)\", s, flags=re.IGNORECASE):\n",
    "                    num = float(s.replace(\"$\", \"\"))\n",
    "                    return num\n",
    "\n",
    "    else: \n",
    "        return np.nan\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_dates(s):\n",
    "    # declared patterns\n",
    "    p1 = r\"(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s*[123]?\\d,\\s*\\d{4}\"\n",
    "    p2 = r\"[123]?\\d\\s*(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s*\\d{4}\"\n",
    "    p3 = r\"(?:January|February|March|April|May|June|July|August|September|October|November|December),?\\s*\\d{4}\"\n",
    "    p4 = r\"(\\d{4})(?!\\S)\"\n",
    "    p5 = r\"(\\d{4}-[01]?\\d-[0123]?\\d)\"\n",
    "    if re.findall(p1, str(s), flags=re.IGNORECASE):  # Month DD, YYYY\n",
    "        spam = re.findall(p1, str(s), flags=re.IGNORECASE)[0]\n",
    "        return spam\n",
    "    elif re.findall(p2, str(s), flags=re.IGNORECASE): # DD Month YYYY\n",
    "        spam = re.findall(p2, str(s), flags=re.IGNORECASE)[0]\n",
    "        return spam\n",
    "    elif re.findall(p3, str(s), flags=re.IGNORECASE): # Month YYYY\n",
    "        spam = re.findall(p3, str(s), flags=re.IGNORECASE)[0]\n",
    "        return spam\n",
    "    elif re.findall(p5, str(s), flags=re.IGNORECASE): # YYYY-MM-DD\n",
    "        spam = re.findall(p5, str(s), flags=re.IGNORECASE)[0]\n",
    "        return spam\n",
    "    elif re.findall(p4, str(s), flags=re.IGNORECASE): # YYYY\n",
    "        spam = re.findall(p4, str(s), flags=re.IGNORECASE)[0]\n",
    "        return spam\n",
    "    else:\n",
    "        return np.nan\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_time(s):\n",
    "    # Declared patterns\n",
    "    p1 = r\"(\\d+)(?:', ')?\\s*min\\.?(?:utes)?\" # 120 minutes, 120 min, 120 min., '120', 'min'\n",
    "    p2 = r\"((?:\\d+\\s*h)?\\s*\\d+m)\"# 1h 48m, 70m\n",
    "    p3 = r\"(\\d+\\s*hours?)\" # one record says \"4 Hours\"\n",
    "    if type(s) == float:\n",
    "        return s\n",
    "    if re.findall(p1, str(s), flags=re.IGNORECASE): # 120 minutes, 120 min, 120 min., '120', 'min'\n",
    "        spam = int(re.findall(p1, str(s), flags=re.IGNORECASE)[0])\n",
    "        return spam\n",
    "    elif re.findall(p2, str(s), flags=re.IGNORECASE): # 1h 48m, 70m\n",
    "        spam = re.findall(p2, str(s), flags=re.IGNORECASE)[0]\n",
    "        egg = int(re.findall(r\"(\\d+)m$\", spam, flags=re.IGNORECASE)[0])\n",
    "        try:\n",
    "            hours = int(re.findall(r\"(\\d+)h\", spam, flags=re.IGNORECASE)[0])*60 \n",
    "            egg+=hours\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        return egg\n",
    "    elif re.findall(p3, str(s), flags=re.IGNORECASE): # 1 record says \"4 hours\" \n",
    "        spam = re.findall(p3, str(s), flags=re.IGNORECASE)[0]\n",
    "        egg = int(re.findall(r\"(\\d+)\\s*h\", spam, flags=re.IGNORECASE)[0])*60\n",
    "        return egg\n",
    "    else:\n",
    "        return float(np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automate the Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Add the function that takes in three arguments;\n",
    "# Wikipedia data, Kaggle metadata, and MovieLens rating data (from Kaggle)\n",
    "\n",
    "def extract_transform_load(wiki_json, kaggle_csv, ratings_csv):\n",
    "    # Read in the kaggle metadata and MovieLens ratings CSV files as Pandas DataFrames.\n",
    "    kaggle_df = pd.read_csv(kaggle_csv, sep=\",\", header=0, low_memory=False)\n",
    "    ratings_df = pd.read_csv(ratings_csv, sep=\",\", header=0)\n",
    "\n",
    "    # Open the read the Wikipedia data JSON file.\n",
    "    with open(wiki_json, 'r') as file:\n",
    "        wiki = json.load(file)\n",
    "        file.close()\n",
    "    \n",
    "    # Write a list comprehension to filter out TV shows.\n",
    "    wiki_movies = [movie for movie in wiki if (('imdb_link' and ('Directed by' or 'Director')) in movie.keys()) and ('No. of episodes' not in movie.keys())]\n",
    "    # Write a list comprehension to iterate through the cleaned wiki movies list\n",
    "    # and call the clean_movie function on each movie. Read in the cleaned movies list as a DataFrame.\n",
    "    \n",
    "    cleaned_movies = [cleaning_movie(movie) for movie in wiki_movies]\n",
    "    wiki_df = pd.DataFrame(cleaned_movies)\n",
    "    \n",
    "    # Write a try-except block to catch errors while extracting the IMDb ID using a regular expression string and\n",
    "    # dropping any imdb_id duplicates. If there is an error, capture and print the exception.\n",
    "    imdb_pattern = r\"(tt\\d{7})\"\n",
    "    try:\n",
    "        wiki_df['imdb_id'] = wiki_df['imdb_link'].str.extract(imdb_pattern)\n",
    "    except TypeError: \n",
    "        print(\"imdb_pattern: TypeError, data cell type not string\")\n",
    "    except KeyError: \n",
    "        print(\"'imdb_id' or 'imdb_link' keys not available in wiki_movies\")\n",
    "    except: \n",
    "        print(\"Differnt Error\")\n",
    "    \n",
    "    wiki_df = pd.DataFrame(wiki_df.drop_duplicates(subset='imdb_id'))\n",
    "        \n",
    "    # Write a list comprehension to keep the columns that don't have null values from the wiki_movies_df DataFrame.\n",
    "    columns_to_keep = [column for column in wiki_df if wiki_df[column].isnull().sum()< len(wiki_df)*0.9]\n",
    "    wiki_df = wiki_df[columns_to_keep]\n",
    "\n",
    "    # Create a variable that will hold the non-null values from the “Box office” column.\n",
    "    box_office = wiki_df['Box office'].dropna()\n",
    "\n",
    "    # Convert the box office data created in Step 8 to string values using the lambda and join functions.\n",
    "    box_office = box_office.apply(lambda x: ''.join(x) if type(x) != str else x)\n",
    "\n",
    "    # Clean the box office column in the wiki_movies_df DataFrame with the parse_dollars function.\n",
    "    wiki_df['box_office'] =  box_office.apply(lambda x: parse_dollars(find_dollars(x)))\n",
    "    \n",
    "    # Clean the budget column in the wiki_movies_df DataFrame.\n",
    "    budget = wiki_df['Budget'].dropna()\n",
    "    budget = budget.apply(lambda x: ''.join(x) if type(x) != str else x)\n",
    "    wiki_df['budget'] = budget.apply(lambda x : parse_dollars(find_dollars(x)))\n",
    "\n",
    "    # Clean the release date column in the wiki_movies_df DataFrame.\n",
    "    release_date = wiki_df['Release date'].dropna()\n",
    "    wiki_df['release_date'] = pd.to_datetime(release_date.apply(lambda x : find_dates(x)), infer_datetime_format=True)\n",
    "\n",
    "    # Clean the running time column in the wiki_movies_df DataFrame.\n",
    "    runtime = wiki_df['Running time'].dropna()\n",
    "    wiki_df['running_time'] = runtime.apply(lambda x:  find_time(x))\n",
    "\n",
    "    #  Clean the Kaggle metadata.\n",
    "    kaggle_df = kaggle_df[kaggle_df['adult'].isin(['True', 'False'])]\n",
    "    kaggle_df['adult'] = kaggle_df['adult'] == 'True'\n",
    "    kaggle_df['video'] = kaggle_df['video'] == 'True'\n",
    "    \n",
    "    kaggle_df['budget'] = pd.to_numeric(kaggle_df['budget'], errors='raise')\n",
    "    kaggle_df['id'] = pd.to_numeric(kaggle_df['id'], errors='raise')\n",
    "    kaggle_df['popularity'] = pd.to_numeric(kaggle_df['popularity'], errors='raise')\n",
    "    kaggle_df['release_date'] = pd.to_datetime(kaggle_df['release_date'])\n",
    "\n",
    "    #  Merged the two DataFrames into the movies DataFrame.\n",
    "    wiki_kaggle = pd.merge(wiki_df, kaggle_df, left_on='imdb_id', right_on='imdb_id', how='inner', suffixes=['_wiki', '_kaggle'])\n",
    "    \n",
    "    #  Drop unnecessary columns from the merged DataFrame.\n",
    "    wiki_kaggle = wiki_kaggle.drop(wiki_kaggle[wiki_kaggle['imdb_id'].isna()].index)\n",
    "    wiki_kaggle = wiki_kaggle.drop(['Release date', 'title_wiki', 'Language', 'Box office', 'Running time', 'Budget',\n",
    "                                'Production Comapany', 'year', 'video', \"release_date_wiki\"], axis=1)\n",
    "    \n",
    "    del wiki_df # for imporved memory performace\n",
    "    del kaggle_df # for imporved memory performace\n",
    "    \n",
    "    \n",
    "    #  Add in the function to fill in the missing Kaggle data.\n",
    "    def fill_missing_kaggle_data(df, kaggle_column, wiki_column):\n",
    "        df[kaggle_column] = df.apply(\n",
    "            lambda row: row[wiki_column] if row[kaggle_column] == 0 else row[kaggle_column]\n",
    "            , axis=1)\n",
    "        df.drop(columns=wiki_column, inplace=True)\n",
    "        \n",
    "    #  Call the function in Step 5 with the DataFrame and columns as the arguments.\n",
    "    fill_missing_kaggle_data(wiki_kaggle, 'budget_kaggle', 'budget_wiki')\n",
    "    fill_missing_kaggle_data(wiki_kaggle, 'runtime', 'running_time')\n",
    "    fill_missing_kaggle_data(wiki_kaggle, 'revenue', 'box_office')\n",
    "\n",
    "\n",
    "    #  Filter the movies DataFrame for specific columns.\n",
    "    column_names=  {'url' : 'wikipedia_url', 'Based on' : 'based_on', 'Starring' : \"starring\",\n",
    "                    'Cinematography' : 'cinematography', 'Director': 'director', 'Distributor(s)': 'distributors',\n",
    "                    'Editor(s)': 'editors', 'Producer(s)':'producers', 'Composer(s)':'composers',\n",
    "                    'Writer(s)':'writers', 'id': 'kaggle_id', 'title_kaggle': 'title', 'Country':'country',\n",
    "                    \"budget_kaggle\":\"budget\", \"release_date_kaggle\":\"release_date\" }\n",
    "    wiki_kaggle.rename(column_names, inplace=True, axis='columns')\n",
    "    \n",
    "    #  Rename the columns in the movies DataFrame.\n",
    "    wiki_kaggle.loc[:, ['imdb_id', 'kaggle_id','title','original_title','belongs_to_collection','tagline','wikipedia_url',\n",
    "                'imdb_link','runtime','budget','revenue','release_date','popularity','vote_count', 'vote_average',\n",
    "                 'genres','original_language','overview','spoken_languages','country', 'production_countries',\n",
    "                 'production_companies', 'distributors','producers','director','starring', 'cinematography', 'editors',\n",
    "                 'writers','composers','based_on']]\n",
    "\n",
    "    #  Transform and merge the ratings DataFrame.\n",
    "    rating_df = ratings_df.groupby(['rating', 'movieId'], as_index=False).count()\n",
    "\n",
    "    rating_counts = rating_df.pivot(index=\"movieId\", columns='rating', values=\"userId\")\n",
    "    rating_counts.columns = [f\"rating_{col}\" for col in rating_counts.columns]\n",
    "\n",
    "    \n",
    "    movie_ratings = pd.merge(wiki_kaggle, rating_counts, left_on='kaggle_id', right_index=True, how='left')\n",
    "    movie_ratings[rating_counts.columns] = movie_ratings[rating_counts.columns].fillna(0)\n",
    "    \n",
    "    \n",
    "    \n",
    "### Upload the data\n",
    "    # create connection\n",
    "    db_string = f\"postgresql://postgres:{postgres_password}@localhost:5432/movie_data\"\n",
    "    engine = create_engine(db_string)\n",
    "    \n",
    "    # upload movie_ratings\n",
    "    movie_ratings.to_sql(name='movies', con=engine, if_exists='replace')\n",
    "    \n",
    "    \n",
    "    start_time = time.time()\n",
    "    rows_imported = 0\n",
    "    chunk = 1000000\n",
    "\n",
    "    for data in pd.read_csv(ratings_file, chunksize=chunk):\n",
    "        print(f\"Rows imported so far: {rows_imported + len(data)}\")\n",
    "        data.to_sql('ratings', con=engine, if_exists='append')\n",
    "        rows_imported += len(data)\n",
    "\n",
    "    print(f\"Done: {rows_imported} Successfully Imported and took {time.time() - start_time} seconds\")\n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filepaths for the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the path to your file directory and variables for the three files.\n",
    "\n",
    "# The Wikipedia data\n",
    "wiki_file = os.path.join('Data', 'wikipedia-movies.json')\n",
    "# The Kaggle metadata\n",
    "kaggle_file = os.path.join('Data', 'movies_metadata.csv')\n",
    "# The MovieLens rating data.\n",
    "ratings_file = os.path.join('Data', 'ratings.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows imported so far: 1000000\n",
      "Rows imported so far: 2000000\n",
      "Rows imported so far: 3000000\n",
      "Rows imported so far: 4000000\n",
      "Rows imported so far: 5000000\n",
      "Rows imported so far: 6000000\n",
      "Rows imported so far: 7000000\n",
      "Rows imported so far: 8000000\n",
      "Rows imported so far: 9000000\n",
      "Rows imported so far: 10000000\n",
      "Rows imported so far: 11000000\n",
      "Rows imported so far: 12000000\n",
      "Rows imported so far: 13000000\n",
      "Rows imported so far: 14000000\n",
      "Rows imported so far: 15000000\n",
      "Rows imported so far: 16000000\n",
      "Rows imported so far: 17000000\n",
      "Rows imported so far: 18000000\n",
      "Rows imported so far: 19000000\n",
      "Rows imported so far: 20000000\n",
      "Rows imported so far: 21000000\n",
      "Rows imported so far: 22000000\n",
      "Rows imported so far: 23000000\n",
      "Rows imported so far: 24000000\n",
      "Rows imported so far: 25000000\n",
      "Rows imported so far: 26000000\n",
      "Rows imported so far: 26024289\n",
      "Done: 26024289 Successfully Imported and took 768.7105503082275 seconds\n"
     ]
    }
   ],
   "source": [
    "extract_transform_load(wiki_file, kaggle_file, ratings_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
